{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import open3d as o3d\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import argparse\n",
    "import torch\n",
    "from scipy.spatial import KDTree\n",
    "\n",
    "\n",
    "from read_depth_and_build_pcd import read_depth_image, read_color_image, build_point_cloud_from_depth\n",
    "from cam_settings import cam_series\n",
    "\n",
    "# some camera settings\n",
    "camera_set = {\n",
    "    0 : [\"0385\", \"2543\", \"1246\", \"1973\"],\n",
    "    1 : [\"4320\", \"1040\", \"1634\"],\n",
    "    2 : [\"1705\", \"1318\", \"1100\"],\n",
    "    3 : [\"1285\", \"1753\", \"8540\"],\n",
    "    4 : [\"1116\", \"1265\", \"0103\"],\n",
    "    5 : [\"1169\", \"1516\", \"2448\"],\n",
    "    6 : [\"2129\", \"0028\", \"0244\"],\n",
    "    7 : [\"1228\", \"0879\", \"1362\"],\n",
    "    8 : [\"1171\", \"1000\", \"1487\", \"0406\"],\n",
    "}\n",
    "id2cam = [\"1246\",\"2543\",\"0385\",\"1973\",\"1634\",\"1040\",\"4320\",\"0879\",\"1362\",\"1228\",\"0028\",\"0244\",\"2129\",\"2448\",\"1516\",\"1169\",\"0103\",\"1265\",\"1116\",\"1753\",\"8540\",\"1285\",\"1100\",\"1318\",\"1705\"]\n",
    "cam2id = {}\n",
    "for i, cam in enumerate(id2cam):\n",
    "    cam2id[cam] = i\n",
    "keyframe_list = np.array([240, 220, 200, 175, 145, 110, 80, 50]) # keyframe - each group can capture front view of chessboard\n",
    "verbose = 0\n",
    "\n",
    "\n",
    "# read data from raw and meta data\n",
    "def read_data(root_path, cam_series_id, frame_id, need_depth = True, mask_path = None, near_clip=0.5, far_clip=3.0):\n",
    "    meta_path = os.path.join(root_path, 'meta_data', f'{cam_series_id}-MODEL.json')\n",
    "    frame_str = str(frame_id).zfill(7)\n",
    "    depth_path = os.path.join(root_path, 'raw_data', frame_str, f'{cam_series_id}-DEPTH.{frame_str}.raw')\n",
    "    color_path = depth_path.replace('DEPTH', 'COLOR')\n",
    "    \n",
    "    # read data related to color camera\n",
    "    with open(meta_path, 'r') as f:\n",
    "        meta = json.load(f)\n",
    "    color_intrinsics = meta['color_intrinsics']\n",
    "    color_intrinsic_matrix = np.array(\n",
    "        [[color_intrinsics['fx'], 0, color_intrinsics['ppx']],\n",
    "            [0, color_intrinsics['fy'], color_intrinsics['ppy']],\n",
    "            [0, 0, 1]])\n",
    "    color_height = meta['color_intrinsics']['height']\n",
    "    color_width = meta['color_intrinsics']['width']\n",
    "    \n",
    "    distortion_coeffs = [color_intrinsics['k1'],color_intrinsics['k2'],color_intrinsics['p1'],color_intrinsics['p2'],color_intrinsics['k3']]\n",
    "    distortion_coeffs = np.array(distortion_coeffs)\n",
    "    \n",
    "    rgb_img = read_color_image(color_path, width=color_width, height=color_height, verbose=verbose)\n",
    "    \n",
    "    depth_to_color_warped_pixels = None\n",
    "    depth_to_color_cam_xyz = None\n",
    "    intrinsic_matrix = None\n",
    "    depth = None\n",
    "    depth_clipped = None\n",
    "    depth_scale = None\n",
    "    extrinsics = None\n",
    "    \n",
    "    # read data related to depth camera\n",
    "    if need_depth:\n",
    "        intrinsics = meta['depth_intrinsics']\n",
    "        intrinsic_matrix = np.array([[intrinsics['fx'], 0, intrinsics['ppx']],\n",
    "                                        [0, intrinsics['fy'], intrinsics['ppy']],\n",
    "                                        [0, 0, 1]])\n",
    "        depth = read_depth_image(depth_path, verbose=verbose)\n",
    "        depth_scale = meta['depth_scale']\n",
    "        camera_pts = build_point_cloud_from_depth(\n",
    "                depth, intrinsic_matrix, depth_scale,\n",
    "                near_clip=near_clip, far_clip=far_clip)\n",
    "        depth_clipped = camera_pts[:, -1]\n",
    "        # Align depth with color pixels\n",
    "        # build camera to depth extrinsics transform\n",
    "        color_offset_extrinsics = meta['color_offset_extrinsics']\n",
    "        R = np.array([[color_offset_extrinsics['r00'], color_offset_extrinsics['r01'], color_offset_extrinsics['r02']],\n",
    "                        [color_offset_extrinsics['r10'], color_offset_extrinsics['r11'], color_offset_extrinsics['r12']],\n",
    "                        [color_offset_extrinsics['r20'], color_offset_extrinsics['r21'], color_offset_extrinsics['r22']]])\n",
    "        T = np.array([color_offset_extrinsics['t0'], color_offset_extrinsics['t1'], color_offset_extrinsics['t2']])\n",
    "        extrinsics = np.eye(4)\n",
    "        extrinsics[:3, :3] = R\n",
    "        extrinsics[:3, 3] = T\n",
    "        \n",
    "        # Apply the transform to depth xyz\n",
    "        depth_xyz = camera_pts\n",
    "        # Extend to homogeneous coordinates\n",
    "        depth_xyz = np.hstack((depth_xyz, np.ones((depth_xyz.shape[0], 1))))\n",
    "        # Apply extrinsics and transform to color camera space\n",
    "        depth_to_color_cam_xyz = np.dot(depth_xyz, extrinsics.T)\n",
    "        depth_to_color_cam_xyz = depth_to_color_cam_xyz[:, :3]\n",
    "        \n",
    "        depth_to_color_warped_pixels = np.dot(color_intrinsic_matrix, depth_to_color_cam_xyz.T).T\n",
    "        depth_to_color_warped_pixels = depth_to_color_warped_pixels[:, :] / depth_to_color_warped_pixels[:, 2:]\n",
    "        depth_to_color_warped_pixels = depth_to_color_warped_pixels[:, :2]\n",
    "    \n",
    "    mask = None\n",
    "    if mask_path is not None:\n",
    "        img_id = cam2id[cam_series_id[-4:]]\n",
    "        cam_mask_path = os.path.join(mask_path, f'mask_{img_id:02d}.png')\n",
    "        mask = cv2.imread(cam_mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "        mask = mask > 128\n",
    "        \n",
    "    return dict(\n",
    "        cam_id = cam_series_id,\n",
    "        rgb_img = rgb_img,\n",
    "        intrinsics = color_intrinsic_matrix,\n",
    "        distortion = distortion_coeffs,\n",
    "        height = color_height,\n",
    "        width = color_width,\n",
    "        \n",
    "        depth_img = depth,\n",
    "        depth_clipped = depth_clipped,\n",
    "        depth_scale = depth_scale,\n",
    "        depth_warp_pixels = depth_to_color_warped_pixels,\n",
    "        depth_warp_xyz = depth_to_color_cam_xyz,\n",
    "        depth_intrinsics = intrinsic_matrix,\n",
    "        \n",
    "        color_offset_extrinsics = extrinsics,\n",
    "        mask = mask\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_point_cloud_from_depth_downsampled(info,                              \n",
    "                                 downsample_step = 10,near_clip=0.5,far_clip=2, output_path = 'tmp'):\n",
    "    '''Build point cloud from depth image.\n",
    "    Args:\n",
    "        depth_img: np.ndarray, depth image.\n",
    "        depth_intrinsics_matrix: np.ndarray, 3x3 matrix of depth intrinsics, as specified in the meta data.\n",
    "        depth_scale: float, depth scale, as specified in the meta data.\n",
    "\n",
    "    Returns:\n",
    "        xyz: np.ndarray, point cloud coordinates in the camera space.\n",
    "    '''\n",
    "    depth_img = info['depth_img']\n",
    "    depth_intrinsics_matrix = info['depth_intrinsics']\n",
    "    depth_scale = info['depth_scale']\n",
    "    \n",
    "    depth = depth_img.astype(np.float32)\n",
    "    depth = depth * depth_scale\n",
    "\n",
    "    # build point cloud\n",
    "    HEIGHT, WIDTH = depth_img.shape[:2]\n",
    "    x = np.arange(0, WIDTH)\n",
    "    y = np.arange(0, HEIGHT)\n",
    "    x, y = np.meshgrid(x, y)\n",
    "    # apply intrinsic matrix to get camera space coordinates\n",
    "    x = (x - depth_intrinsics_matrix[0, 2]) * depth / depth_intrinsics_matrix[0, 0]\n",
    "    y = (y - depth_intrinsics_matrix[1, 2]) * depth / depth_intrinsics_matrix[1, 1]\n",
    "    \n",
    "    downsample_mask = np.zeros_like(depth, dtype=bool)\n",
    "    downsample_mask[::downsample_step, ::downsample_step] = True\n",
    "    clip_mask = (depth >= near_clip) & (depth <= far_clip)\n",
    "    dc_mask = clip_mask & downsample_mask\n",
    "    \n",
    "    x = x[dc_mask].flatten()\n",
    "    y = y[dc_mask].flatten()\n",
    "    z = depth[dc_mask].flatten()\n",
    "    xyz_downsampled = np.vstack((x, y, z)).T\n",
    "    xyz_downsampled = xyz_downsampled.astype(np.float32)\n",
    "    # xyz = xyz.astype(np.float32)\n",
    "    \n",
    "    offset_extrinsics = info['color_offset_extrinsics']\n",
    "    color_intrinsic_matrix = info['intrinsics']\n",
    "\n",
    "    xyz_homogeneous = np.hstack((xyz_downsampled, np.ones((xyz_downsampled.shape[0], 1))))\n",
    "    # Apply extrinsics and transform to color camera space\n",
    "    depth_to_color_cam_xyz = np.matmul(xyz_homogeneous, offset_extrinsics.T)\n",
    "    depth_to_color_cam_xyz = depth_to_color_cam_xyz[:, :3]\n",
    "    \n",
    "    depth_to_color_warped_pixels = np.dot(color_intrinsic_matrix, depth_to_color_cam_xyz.T).T\n",
    "    depth_to_color_warped_pixels = depth_to_color_warped_pixels[:, :] / depth_to_color_warped_pixels[:, 2:]\n",
    "    depth_to_color_warped_pixels = depth_to_color_warped_pixels[:, :2]\n",
    "    \n",
    "    depth_to_color_warped_pixels_int = depth_to_color_warped_pixels.astype(int)\n",
    "    \n",
    "    y_coords = depth_to_color_warped_pixels_int[:, 1]\n",
    "    x_coords = depth_to_color_warped_pixels_int[:, 0]\n",
    "    \n",
    "\n",
    "    image = info['rgb_img']\n",
    "    img_mask = info['mask']\n",
    "    valid_mask = (x_coords >= 0) & (x_coords < image.shape[1]) & (y_coords >= 0) & (y_coords < image.shape[0])\n",
    "    # Combine valid_mask with img_mask\n",
    "    if img_mask is not None:\n",
    "        valid_mask = valid_mask & img_mask[y_coords, x_coords]\n",
    "\n",
    "    colors = image[y_coords[valid_mask], x_coords[valid_mask]]\n",
    "    \n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    \n",
    "    pcd.points = o3d.utility.Vector3dVector(depth_to_color_cam_xyz[valid_mask])\n",
    "    pcd.colors = o3d.utility.Vector3dVector(colors / 255.)\n",
    "    \n",
    "    cam_id = info['cam_id']\n",
    "    if not os.path.exists(output_path):\n",
    "        os.mkdir(output_path)\n",
    "    o3d.io.write_point_cloud(os.path.join(output_path,f'pcd-{cam_id}.ply'), pcd)\n",
    "    return pcd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/scratch/projects/fouheylab/dma9300/recon3d/data/cali_1/raw_data/0000220/043422251634-COLOR.0000220.raw'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cam \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(cam_series\u001b[38;5;241m.\u001b[39mkeys()):\n\u001b[1;32m     11\u001b[0m     test_serie \u001b[38;5;241m=\u001b[39m cam_series[cam]\n\u001b[0;32m---> 12\u001b[0m     info \u001b[38;5;241m=\u001b[39m \u001b[43mread_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_serie\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     build_point_cloud_from_depth_downsampled(info, far_clip\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, output_path\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_path, frame_id), downsample_step\u001b[38;5;241m=\u001b[39mdownsample_step)\n",
      "Cell \u001b[0;32mIn[1], line 56\u001b[0m, in \u001b[0;36mread_data\u001b[0;34m(root_path, cam_series_id, frame_id, need_depth, mask_path, near_clip, far_clip)\u001b[0m\n\u001b[1;32m     53\u001b[0m distortion_coeffs \u001b[38;5;241m=\u001b[39m [color_intrinsics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mk1\u001b[39m\u001b[38;5;124m'\u001b[39m],color_intrinsics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mk2\u001b[39m\u001b[38;5;124m'\u001b[39m],color_intrinsics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mp1\u001b[39m\u001b[38;5;124m'\u001b[39m],color_intrinsics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mp2\u001b[39m\u001b[38;5;124m'\u001b[39m],color_intrinsics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mk3\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[1;32m     54\u001b[0m distortion_coeffs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(distortion_coeffs)\n\u001b[0;32m---> 56\u001b[0m rgb_img \u001b[38;5;241m=\u001b[39m \u001b[43mread_color_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolor_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwidth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolor_width\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolor_height\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m depth_to_color_warped_pixels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     59\u001b[0m depth_to_color_cam_xyz \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/scratch/projects/fouheylab/dma9300/recon3d/read_depth_and_build_pcd.py:39\u001b[0m, in \u001b[0;36mread_color_image\u001b[0;34m(color_path, width, height, verbose)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_color_image\u001b[39m(color_path, width\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1280\u001b[39m, height\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m800\u001b[39m,\n\u001b[1;32m     33\u001b[0m                      verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m     34\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''Read color image from raw file and convert to RGB.\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;124;03m        color_path: str, path to the raw color image.\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;124;03m        width: int, width of the color image.\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;124;03m        height: int, height of the color image.'''\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m     raw_img \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfromfile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolor_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muint8\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mreshape(height, width, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     40\u001b[0m     bgr_img \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(raw_img, cv2\u001b[38;5;241m.\u001b[39mCOLOR_YUV2BGR_YUY2)\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m verbose:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/scratch/projects/fouheylab/dma9300/recon3d/data/cali_1/raw_data/0000220/043422251634-COLOR.0000220.raw'"
     ]
    }
   ],
   "source": [
    "# build pcd directly\n",
    "root_path = '/scratch/projects/fouheylab/dma9300/recon3d/data/cali_1'\n",
    "output_path = 'tmp_dense'\n",
    "downsample_step = 4\n",
    "if not os.path.exists(output_path):\n",
    "    os.mkdir(output_path)\n",
    "for i in range(len(keyframe_list)):\n",
    "    \n",
    "    frame_id = str(keyframe_list[i]).zfill(7)\n",
    "    for cam in list(cam_series.keys()):\n",
    "        test_serie = cam_series[cam]\n",
    "        info = read_data(root_path, test_serie, frame_id)\n",
    "        build_point_cloud_from_depth_downsampled(info, far_clip=2, output_path=os.path.join(output_path, frame_id), downsample_step=downsample_step)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build masked pcd\n",
    "root_path = 'data'\n",
    "output_path = os.path.join('data', 'pcd_data')\n",
    "frame_id = str(50).zfill(7)\n",
    "cam_list = list(cam_series.keys())\n",
    "cam_list = [cam for cam in cam_list if cam not in camera_set[8]]\n",
    "for cam in cam_list:\n",
    "    test_serie = cam_series[cam]\n",
    "    info = read_data(root_path, cam_series[cam], frame_id, need_depth=True, mask_path=os.path.join('data', 'masked_imgs','masks'))\n",
    "    build_point_cloud_from_depth_downsampled(info, far_clip=2, downsample_step=2, output_path=os.path.join('data', 'masked_pcd_data',))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_sum(distances, coor_3d):\n",
    "    dist = distances.reshape((-1,1))\n",
    "    weights = 1 / (dist + 1e-8) \n",
    "    weighted_coor = np.sum(weights * coor_3d, axis=0)\n",
    "    average_coor = weighted_coor / np.sum(weights)\n",
    "    return average_coor\n",
    "\n",
    "# calibrate all source cameras to target camera\n",
    "def camera_calibration(root_path, source_cam, target_cam, frame_id, k=4):\n",
    "    frame_id = str(frame_id).zfill(7)\n",
    "    target_info = read_data(root_path, target_cam, frame_id)\n",
    "    \n",
    "    # find chessboard corners in target image\n",
    "    cb_size = (7,4)\n",
    "    gray_target_img = cv2.cvtColor(target_info['rgb_img'], cv2.COLOR_BGR2GRAY)\n",
    "    ret, corners = cv2.findChessboardCorners(gray_target_img, cb_size, None)\n",
    "    if not ret:\n",
    "        print(f'Error! No chessboard detected in target {target_cam} on frame {frame_id}. Quit.')\n",
    "        return\n",
    "    corners = corners.squeeze()\n",
    "    \n",
    "    # calculate obj points in target space\n",
    "    tree = KDTree(target_info['depth_warp_pixels'])\n",
    "    obj_points = []\n",
    "    for new_point in corners:\n",
    "        distances, indices = tree.query(new_point, k=k)\n",
    "        coor_3d = target_info['depth_warp_xyz'][indices]\n",
    "        obj_points.append(weighted_sum(distances, coor_3d))\n",
    "    obj_points = np.array(obj_points)\n",
    "    \n",
    "    # TBA: check rvec and tvec\n",
    "    \n",
    "    source_mat = {}\n",
    "    for s_cam in source_cam:\n",
    "        # find chessboard corners in source image\n",
    "        source_info = read_data(root_path, s_cam, frame_id, False)\n",
    "        gray_source_img = cv2.cvtColor(source_info['rgb_img'], cv2.COLOR_BGR2GRAY)\n",
    "        ret, source_corners = cv2.findChessboardCorners(gray_source_img, cb_size, None)\n",
    "        if not ret:\n",
    "            print(f'Camera {s_cam} detect chessboard in frame {frame_id} fail! Continue.')\n",
    "            continue\n",
    "        # solve pnp to get rvec and tvec   \n",
    "        success, rvec, tvec = cv2.solvePnP(obj_points, source_corners, source_info['intrinsics'], source_info['distortion'])\n",
    "        if success:\n",
    "            homo_mat = np.eye(4)\n",
    "            R, _ = cv2.Rodrigues(rvec)\n",
    "            # note: camera to world(1246)\n",
    "            homo_mat[:3, :] = np.hstack((R, tvec))\n",
    "            source_mat[s_cam[-4:]] = np.linalg.inv(homo_mat)\n",
    "    trans_dict = dict(\n",
    "        target = target_cam[-4:],\n",
    "        trans_mat = source_mat\n",
    "    )\n",
    "    return trans_dict\n",
    "    \n",
    "def save_trans_dict(td, wb_path):\n",
    "    if not os.path.exists(wb_path):\n",
    "        os.mkdir(wb_path)\n",
    "    target_id = td['target']\n",
    "    trans_dict = td['trans_mat']\n",
    "    for sid in trans_dict.keys():\n",
    "        trans_mat = trans_dict[sid]\n",
    "        trans_fn = f'{sid}_to_{target_id}_H_fine.txt'\n",
    "        with open(os.path.join(wb_path, trans_fn), 'w') as wb_f:\n",
    "            np.savetxt(wb_f, trans_mat)\n",
    "        \n",
    "cali_sequence = []\n",
    "for i in range(8):\n",
    "    cali_sequence.append(\n",
    "        dict(\n",
    "            target = camera_set[i][-2],\n",
    "            frame = keyframe_list[i],\n",
    "            source = camera_set[i] + camera_set[(i+1) % 8] + camera_set[(i-1) % 8]\n",
    "        )\n",
    "    )\n",
    "    \n",
    "root_path = 'data'\n",
    "output_path = 'data/output_data'\n",
    "if not os.path.exists(output_path):\n",
    "    os.mkdir(output_path)\n",
    "for cali_set in cali_sequence:\n",
    "    source_series = [cam_series[i] for i in cali_set['source']]\n",
    "    target_serie = cam_series[cali_set['target']]\n",
    "    trans_dict = camera_calibration(root_path, source_series, target_serie, frame_id=cali_set['frame'], k=4)\n",
    "    save_trans_dict(trans_dict, output_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiply two transformation matrix to get a new one\n",
    "def trans_mat_mul(root_path, a_id, b_id, c_id):  \n",
    "    a2b_fn = os.path.join(root_path, f'{a_id}_to_{b_id}_H_fine.txt')\n",
    "    b2c_fn = os.path.join(root_path, f'{b_id}_to_{c_id}_H_fine.txt')\n",
    "    a2c_fn = os.path.join(root_path, f'{a_id}_to_{c_id}_H_fine.txt')\n",
    "    \n",
    "    if os.path.exists(a2c_fn):\n",
    "        print(f'{a_id} to {c_id} transform exist. Finish.')\n",
    "        return True\n",
    "    \n",
    "    if not os.path.exists(a2b_fn) or not os.path.exists(b2c_fn):\n",
    "        return False\n",
    "    \n",
    "    a2b_mat = np.loadtxt(a2b_fn)\n",
    "    b2c_mat = np.loadtxt(b2c_fn)\n",
    "    \n",
    "    a2c_mat = np.dot(b2c_mat, a2b_mat)\n",
    "    \n",
    "    np.savetxt(a2c_fn, a2c_mat, fmt='%e')\n",
    "    return True\n",
    "\n",
    "\n",
    "trans_path = 'data/output_data'\n",
    "target_id = '1246'\n",
    "raw_cam_list = list(cam_series.keys())\n",
    "cali_cam_list = camera_set[0] # have xxx_to_1246 directly\n",
    "raw_cam_list = [cam for cam in raw_cam_list if cam not in cali_cam_list + camera_set[8]]\n",
    "\n",
    "# calibrate all cameras to central camera(1246)\n",
    "while len(raw_cam_list):\n",
    "    trans_list = os.listdir(trans_path)\n",
    "    trans_list = [t.split('_') for t in trans_list]\n",
    "    trans_list = [(t[0], t[2]) for t in trans_list]\n",
    "    \n",
    "    rm_list = []\n",
    "    for raw_cam in raw_cam_list:\n",
    "        for trans in trans_list:\n",
    "            if trans[0] == raw_cam and trans[1] in cali_cam_list:\n",
    "                trans_mat_mul(trans_path, raw_cam, trans[1], target_id)\n",
    "                rm_list.append(raw_cam)\n",
    "                break\n",
    "    raw_cam_list = [cam for cam in raw_cam_list if cam not in rm_list]\n",
    "    cali_cam_list.extend(rm_list)\n",
    "    print(f'Current left : {len(raw_cam_list)} cameras.')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# register local pcds to world view\n",
    "import copy\n",
    "\n",
    "def read_pcd_and_trans(cam_id, pcd_path, trans_path, target_id='1246'):\n",
    "    trans_fn = os.path.join(trans_path, f'{cam_id}_to_{target_id}_H_fine.txt')\n",
    "    cam_serie = cam_series[cam_id]\n",
    "    pcd_fn = os.path.join(pcd_path, f'pcd-{cam_serie}.ply')\n",
    "    pcd = o3d.io.read_point_cloud(pcd_fn)\n",
    "    trans = np.loadtxt(trans_fn) if cam_id != target_id else np.eye(4)\n",
    "    return dict(\n",
    "        pcd = pcd,\n",
    "        trans = trans\n",
    "    )\n",
    "\n",
    "def save_registration_result_color_multi(sources, target, transformations, output_path):\n",
    "    geometries = [copy.deepcopy(target)]\n",
    "    combined_point_cloud = copy.deepcopy(target)\n",
    "    transformations = np.array(transformations)\n",
    "    if transformations.ndim == 2:\n",
    "        transformations = np.expand_dims(transformations, axis=0)\n",
    "\n",
    "    for i in range(len(sources)):\n",
    "        source_temp = copy.deepcopy(sources[i])\n",
    "        source_temp.transform(transformations[i])\n",
    "        geometries.append(copy.deepcopy(source_temp))\n",
    "        combined_point_cloud += source_temp\n",
    "\n",
    "    o3d.io.write_point_cloud(output_path, combined_point_cloud)\n",
    "\n",
    "frame_id = str(50).zfill(7)\n",
    "pcd_path = f'tmp_dense/{frame_id}'\n",
    "trans_path = 'data/output_data'\n",
    "target_id = '1246'\n",
    "source_id = [cam_id for cam_id in list(cam_series.keys()) if cam_id not in camera_set[8]]\n",
    "target_input = read_pcd_and_trans(target_id, pcd_path, trans_path)\n",
    "source_input = [read_pcd_and_trans(sid, pcd_path, trans_path, target_id) for sid in source_id]\n",
    "source_pcd = [sinfo['pcd'] for sinfo in source_input]\n",
    "source_trans = [sinfo['trans'] for sinfo in source_input]\n",
    "save_registration_result_color_multi(source_pcd, target_input['pcd'],source_trans, f'data/combined_pcd_{target_id}.ply')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
